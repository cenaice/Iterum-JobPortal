{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd14ebb2-983e-45cb-a628-e6930faf2e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Front End Dev: 0.7859848484848485\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 129\u001b[0m\n\u001b[1;32m    125\u001b[0m     exp\u001b[38;5;241m.\u001b[39mshow_in_notebook()\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(exp\u001b[38;5;241m.\u001b[39mas_list())\n\u001b[0;32m--> 129\u001b[0m \u001b[43mLIME_explainability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_fe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfront_end_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer_fe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNot Front End Dev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFront End Dev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m shap_plot \u001b[38;5;241m=\u001b[39m shap_explainability(X_test_fe, front_end_model, vectorizer_fe)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# LIME_explainability(X_test_be, back_end_model)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# LIME_explainability(X_test_ai, ai_ml_model)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 123\u001b[0m, in \u001b[0;36mLIME_explainability\u001b[0;34m(X_test, model, vectorizer, class_names)\u001b[0m\n\u001b[1;32m    121\u001b[0m explainer \u001b[38;5;241m=\u001b[39m LimeTextExplainer(class_names\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[1;32m    122\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Ensure this index is valid in X_test\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlime_predict_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m exp\u001b[38;5;241m.\u001b[39mshow_in_notebook()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(exp\u001b[38;5;241m.\u001b[39mas_list())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lime/lime_text.py:413\u001b[0m, in \u001b[0;36mLimeTextExplainer.explain_instance\u001b[0;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    406\u001b[0m indexed_string \u001b[38;5;241m=\u001b[39m (IndexedCharacters(\n\u001b[1;32m    407\u001b[0m     text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow, mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string)\n\u001b[1;32m    408\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_level \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    409\u001b[0m                   IndexedString(text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow,\n\u001b[1;32m    410\u001b[0m                                 split_expression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_expression,\n\u001b[1;32m    411\u001b[0m                                 mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string))\n\u001b[1;32m    412\u001b[0m domain_mapper \u001b[38;5;241m=\u001b[39m TextDomainMapper(indexed_string)\n\u001b[0;32m--> 413\u001b[0m data, yss, distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__data_labels_distances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexed_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(yss[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lime/lime_text.py:482\u001b[0m, in \u001b[0;36mLimeTextExplainer.__data_labels_distances\u001b[0;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[1;32m    480\u001b[0m     data[i, inactive] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    481\u001b[0m     inverse_data\u001b[38;5;241m.\u001b[39mappend(indexed_string\u001b[38;5;241m.\u001b[39minverse_removing(inactive))\n\u001b[0;32m--> 482\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverse_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m distances \u001b[38;5;241m=\u001b[39m distance_fn(sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix(data))\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, labels, distances\n",
      "Cell \u001b[0;32mIn[4], line 111\u001b[0m, in \u001b[0;36mpredict_proba_wrapper.<locals>.predict_proba\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(X):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Transform the input using the vectorizer and predict using the model\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/pipeline.py:584\u001b[0m, in \u001b[0;36mPipeline.predict_proba\u001b[0;34m(self, X, **predict_proba_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 584\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict_proba(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_proba_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2163\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \n\u001b[1;32m   2148\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2161\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2163\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1434\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1434\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1436\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from target_variables import *\n",
    "from text_preprocess import preprocess_text\n",
    "import pandas as pd\n",
    "import shap\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "file_path_dice = '../../../datasets/dice_com-job_us_sample.csv'\n",
    "dice_data = pd.read_csv(file_path_dice)\n",
    "\n",
    "# Randomly sample a subset of the data (100% in this case)\n",
    "sampled_data = dice_data.sample(frac=0.75, random_state=1)\n",
    "\n",
    "# Remove duplicates\n",
    "sampled_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Fill missing values with 'unknown'\n",
    "sampled_data.fillna('unknown', inplace=True)\n",
    "\n",
    "# Select relevant columns and preprocess text\n",
    "# Update these column names based on the actual columns in your dataset, change according to dataset\n",
    "sampled_data = sampled_data[[\n",
    "    'jobtitle', 'jobdescription', 'employmenttype_jobstatus', 'skills']]\n",
    "\n",
    "# Apply preprocessing to text columns\n",
    "for column in ['jobtitle', 'jobdescription', 'employmenttype_jobstatus', 'skills']:\n",
    "    sampled_data[column] = sampled_data[column].apply(preprocess_text)\n",
    "\n",
    "# Concatenate text features into a single feature for analysis\n",
    "sampled_data['combined_text'] = sampled_data['jobtitle'] + ' ' + sampled_data['jobdescription'] + \\\n",
    "    ' ' + sampled_data['employmenttype_jobstatus'] + \\\n",
    "    ' ' + sampled_data['skills']\n",
    "\n",
    "# Apply the function to create the target variable\n",
    "sampled_data['is_front_end_dev'] = sampled_data.apply(\n",
    "    lambda x: JobCategoryClassifier.is_front_end_developer(x['jobtitle'], x['jobdescription']), axis=1)\n",
    "\n",
    "sampled_data['is_back_end_dev'] = sampled_data.apply(\n",
    "    lambda x: JobCategoryClassifier.is_back_end_developer(x['jobtitle'], x['jobdescription']), axis=1)\n",
    "\n",
    "sampled_data['is_ai_ml'] = sampled_data.apply(\n",
    "    lambda x: JobCategoryClassifier.is_ai_ml(x['jobtitle'], x['jobdescription']), axis=1)\n",
    "\n",
    "\n",
    "def training_model(X, y, class_name):\n",
    "    \"\"\"\n",
    "    Trains our model based on the given datasets. Adjust test sets accordingly\n",
    "\n",
    "    Parameters:\n",
    "    X : Concatenated text features into a single feature for analysis\n",
    "    y : Target Variable\n",
    "\n",
    "    Prints:\n",
    "    Accuracy Score of Model\n",
    "\n",
    "    Returns:\n",
    "    Trained model, X_test, y_test.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.8, random_state=1)\n",
    "\n",
    "    # Create a pipeline that transforms the data using TF-IDF and then fits the Naive Bayes classifier\n",
    "    model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Extract the trained vectorizer\n",
    "    vectorizer = model.named_steps['tfidfvectorizer']\n",
    "\n",
    "    print(f\"Accuracy of {class_name}:\", accuracy_score(\n",
    "        y_test, model.predict(X_test)))\n",
    "\n",
    "    return model, vectorizer, X_test, y_test\n",
    "\n",
    "\n",
    "front_end_model, vectorizer_fe, X_test_fe, y_test_fe = training_model(\n",
    "    sampled_data['combined_text'], sampled_data['is_front_end_dev'], \"Front End Dev\")\n",
    "\n",
    "# back_end_model, X_test_be, y_test_be = training_model(sampled_data['combined_text'], sampled_data['is_back_end_dev'], \"Back End Dev\")\n",
    "\n",
    "# ai_ml_model, X_test_ai, y_test_ai = training_model(sampled_data['combined_text'], sampled_data['is_ai_ml'], \"AI ML\")\n",
    "\n",
    "\n",
    "def shap_explainability(X, model, vectorizer):\n",
    "    shap_predict_proba = predict_proba_wrapper(model, vectorizer, X)\n",
    "    X_transformed = vectorizer.transform(X)\n",
    "\n",
    "    explainer = shap.Explainer(shap_predict_proba, X_transformed)\n",
    "    shap_values = explainer(X_transformed)\n",
    "\n",
    "    # Visualize the SHAP values for a specific prediction\n",
    "    shap.initjs()\n",
    "    return shap.force_plot(explainer.expected_value[0], shap_values[0])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# shap_plot = shap_explainability(X_test_fe, front_end_model)\n",
    "\n",
    "\n",
    "def predict_proba_wrapper(model, vectorizer):\n",
    "    def predict_proba(X):\n",
    "        # Transform the input using the vectorizer and predict using the model\n",
    "        return model.predict_proba(vectorizer.transform(X))\n",
    "    return predict_proba\n",
    "\n",
    "# Use this wrapper in LIME\n",
    "\n",
    "\n",
    "def LIME_explainability(X_test, model, vectorizer, class_names):\n",
    "    # Create a predict_proba function that LIME can use\n",
    "    lime_predict_proba = predict_proba_wrapper(model, vectorizer)\n",
    "\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    idx = 1  # Ensure this index is valid in X_test\n",
    "    exp = explainer.explain_instance(\n",
    "        X_test[idx], lime_predict_proba, num_features=10)\n",
    "    exp.show_in_notebook()\n",
    "    print(exp.as_list())\n",
    "\n",
    "\n",
    "LIME_explainability(X_test_fe, front_end_model, vectorizer_fe, [\n",
    "                    'Not Front End Dev', 'Front End Dev'])\n",
    "\n",
    "shap_plot = shap_explainability(X_test_fe, front_end_model, vectorizer_fe)\n",
    "# LIME_explainability(X_test_be, back_end_model)\n",
    "# LIME_explainability(X_test_ai, ai_ml_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6e41b-b8b6-4614-a11a-a94cc027e7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
